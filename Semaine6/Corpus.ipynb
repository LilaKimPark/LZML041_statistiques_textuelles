{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0f26fb-8a7c-4439-8a68-fdc2c5fde9cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0 - étape 1: Installation NLTK et librairies de visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7083b1-3e44-4560-896b-501186e1fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 1... installer NLKT si vous ne l'avez pas déjà fait \n",
    "#!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846728a6-bcea-494e-b308-107d0df3d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1... import the library NLKT\n",
    "# A new window should open, showing the NLTK Downloader. Click on the File menu and select Change Download Directory. \n",
    "# For central installation, set this to C:\\nltk_data (Windows), /usr/local/share/nltk_data (Mac), or /usr/share/nltk_data (Unix). \n",
    "# Next, download all.\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba3d3e-28fb-4236-8053-c6915be56a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2....installer visualisation libraries\n",
    "!{sys.executable} -m pip install networkx[default]\n",
    "!{sys.executable} -m pip install pyvis\n",
    "!{sys.executable} -m pip install textblob\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c9cc5-4cbd-4868-8304-9474eb82fb56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 - Corpus - example 1 : Wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf4cbd-e870-4ff0-9d6c-e7795ebee4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage for wordnet\n",
    "# WordNet is just a NLTK corpus reader, and can be imported like this:\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70156302-9961-4be7-aec9-68d4679eb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"lemme\"\n",
    "synset = wn.synsets(word, lang='fra')\n",
    "print('The test word is : ', word)\n",
    "print('Word and Type : ' + synset[0].name())\n",
    "print('Synonym is: ' + synset[0].lemmas()[0].name())\n",
    "print('The meaning of the word : ' + synset[0].definition())\n",
    "print('Example : ' + str(synset[0].examples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeb7bc-78c4-4315-b0f8-6403eb5e5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "noeud = Word(\"word\")\n",
    "print (noeud.synsets[:10])\n",
    "print (noeud.definitions[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5489ad-ae6a-45a6-8a96-2778c613bcab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G=nx.Graph()\n",
    "\n",
    "w=noeud.synsets[0]\n",
    "\n",
    "G.add_node(w.name())\n",
    "for h in w.hypernyms():\n",
    "    #print (h)\n",
    "    G.add_node(h.name())\n",
    "    G.add_edge(w.name(),h.name())\n",
    "\n",
    "\n",
    "for h in w.hyponyms():\n",
    "    #print (h)\n",
    "    G.add_node(h.name())\n",
    "    G.add_edge(w.name(),h.name())\n",
    "\n",
    "print (G.nodes(data=True))\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = [21, 5]\n",
    "nx.draw(G, width=1, with_labels=True, node_color=\"#007ed9\")\n",
    "plt.savefig(\"path.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd849e8-4c65-426d-b66a-0e4cc0494845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see all other layouts: https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.random_layout.html\n",
    "nx.draw(G, pos=nx.spiral_layout(G))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284151ca-31ee-45ef-9731-31c5ae123cb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - Corpus - example 2 : Guttenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed669a-5bea-4640-b008-be41804494b2",
   "metadata": {},
   "source": [
    "## a - Télécharger un livre du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33632eb-9db2-4564-9643-abace604282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore the Guttenberg in NLKT !\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fa038-7da3-4a2f-a361-22f018791027",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick = nltk.corpus.gutenberg.words( 'melville-moby_dick.txt')\n",
    "len(moby_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d41410-e269-41b3-93f7-11bae84fcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in nltk.corpus.gutenberg.fileids():\n",
    "    print('# of words in ',text,'is: ', len(nltk.corpus.gutenberg.words( text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006f44e-3e0e-4ae7-aa96-728189205250",
   "metadata": {},
   "source": [
    "## b - Télécharger un livre du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbcde9-1083-47e5-9514-f6ae77edda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load a specific book from Guttenberg website \n",
    "# you will need to leverage the requests package\n",
    "import requests\n",
    "#choose a book in Gutenberg project website the https://www.gutenberg.org/ebooks/5258 and get the reference number of the book, here 5258 !\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/5258/pg5258.txt')\n",
    "Zarathoustra_Nietzsche = r.text\n",
    "\n",
    "# first, remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "    Zarathoustra_Nietzsche = Zarathoustra_Nietzsche.replace(char, \" \")\n",
    "#print number of characters in the book\n",
    "print(len(Zarathoustra_Nietzsche))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebed35-bfe4-43ad-b036-ab35ddbfc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the project gutenburg introduction and footnotes\n",
    "print(Zarathoustra_Nietzsche[0:910]) \n",
    "print('-------------------------------------------------') \n",
    "print(Zarathoustra_Nietzsche[637986:639986]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86566f-a51a-4438-a044-84b764220280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also subset for the book text\n",
    "# (removing the project gutenburg introduction/footnotes)\n",
    "Zarathoustra_Nietzsche = Zarathoustra_Nietzsche[911:637986]\n",
    "#print(Zarathoustra_Nietzsche)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c6e95-07ed-4efe-a600-284104f0cacb",
   "metadata": {},
   "source": [
    "## b- Explorer le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b12172-37cf-474e-9df5-2dba7c702250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Tokenize the Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "# Choose your Text\n",
    "text = Zarathoustra_Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687cd3d-6e01-448b-8153-536111ee5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text, language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a05c4-a370-459f-8e5e-c27a6d13d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 20 most commons tokens\n",
    "from collections import Counter\n",
    "print(Counter(tokens).most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e262d-cd52-4d2c-97e5-602853fd5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove punctiation !\n",
    "remove = re.sub(r'[^\\w\\s]', '', text)\n",
    "#print(\"updated text with no punctuations :\", remove)\n",
    "tokens = word_tokenize(remove, language=\"french\")\n",
    "print(Counter(tokens).most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344a87f-5034-4868-8bcb-d6610e2c364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=100,\n",
    "                      random_state=1, background_color='White', colormap='cubehelix',\n",
    "                      collocations=False, stopwords = STOPWORDS).generate(text)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a68002-8a2c-4412-b720-97d245b69ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove Stopwords !\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "filtre_stopfr = lambda text: [token for token in text if token.lower() not in french_stopwords]\n",
    "\n",
    "tokens_Filtered=filtre_stopfr( tokens)\n",
    "print(Counter(tokens_Filtered).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc803de5-b9d7-4a40-a667-e71bce661a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=100,\n",
    "                      random_state=1, background_color='Black', colormap='Paired',\n",
    "                      collocations=False, stopwords = french_stopwords).generate(text)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f4ac6-c6d4-4b80-9cb3-6209ab94f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "folder = \"/Users/loubnaserrar/0-Cours Text mining\"\n",
    "\n",
    "data = Image.open(os.path.join(folder, \"Nietzsche.jpg\"),'r')\n",
    "#mask = np.array(Image.open(\"Nietzsche.jpg\"))\n",
    "mask = np.array(data)\n",
    "\n",
    "mask.shape\n",
    "# Generating colors from image\n",
    "image_colors = ImageColorGenerator(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39ef12-3ca2-44a5-bb16-55dabb288d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 800, max_words=700,\n",
    "                      random_state=1, background_color='Lightblue', colormap='winter_r',\n",
    "                      collocations=False, stopwords = french_stopwords, mask = mask).generate(text)\n",
    "plt.figure(figsize=(5, 13))\n",
    "plt.imshow(wordcloud) \n",
    "#plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation ='bilinear') # Using the color function to use the image colors\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Pour afficher l'image mask...\n",
    "plt.imshow(mask) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537f5cd-a964-4a67-a32d-4293df7ec083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you wantg to exclude anything else?\n",
    "Stop_words=['plus']\n",
    "for x in Stop_words:\n",
    "    french_stopwords.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af59814-dc03-4f73-957c-a993dec3926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_Filtered=filtre_stopfr( tokens)\n",
    "print(Counter(tokens_Filtered).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7ce72-81e5-4894-a288-d3951b9099cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3 - Corpus example 4 : Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4255c5-8c94-4589-8845-947719fb0bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Web scraping : construire votre corpus directement de Reddit\n",
    "PRAW, an acronym for \"Python Reddit API Wrapper\", is a Python package that allows for simple access to Reddit's API. PRAW aims to be easy to use and internally follows all of Reddit's API rules. With PRAW there's no need to introduce sleep calls in your code. Give your client an appropriate user agent and you're set.\n",
    "\n",
    "#https://scrapingrobot.com/blog/web-scraping-reddit/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8231e-3355-4ab1-9c26-7e22d3374368",
   "metadata": {},
   "source": [
    "### 1 - Installer PRAW : Python Reddit API Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdf5d2-7e58-4c5f-a6c1-da830e4c052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw\n",
    "\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b52994-7ca1-4d32-b901-6eb78f8e2049",
   "metadata": {},
   "source": [
    "### 2 - créer une application sur le site Reddit\n",
    "--> you need to create or log into your account on Reddit.\n",
    "\n",
    "--> click on “Are you a developer? Create an app…”\n",
    "\n",
    "https://www.reddit.com/prefs/apps\n",
    "\n",
    "Suivre le Tuto suivant :\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raddit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1efd21-f786-4872-a966-79f53faad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noter les identifiants de votre apploication personnelle\n",
    "my_client_id = \"\"\n",
    "\n",
    "my_client_secret = \"\"\n",
    "\n",
    "my_user_agent = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240ed24-ec64-41a6-953e-f3f8665a7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=my_client_id, client_secret=my_client_secret, user_agent=my_user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb6e5c-ab82-44bf-bf3e-d73e4bd83b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 hot posts from the MachineLearning subreddit\n",
    "hot_posts = reddit.subreddit('ChatGPT').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae9561-bfbb-46a3-a6cd-5b5ec66c6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hottest posts from all subreddits\n",
    "hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9428ed9-1a30-488c-8477-d98fdb28d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('ChatGPT')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f9b66-449c-4052-9074-26cbfba93099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37399e4c-1ebe-46f5-8d36-38dab2cfe587",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Option B : télécharger votre coprus que vous avez déjà préparer !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb0853-bc9f-4e65-8ad9-6e4baec27120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Dataset\n",
    "import pandas as pd\n",
    "# Attention changer le chemin d'accées à votre fichier\n",
    "df = pd.read_csv('D:/Lila/Download/tweets_Greve_2023.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a96ac-3021-4aab-8f1b-994ec0ae11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2baa7e5-4f67-4668-b608-7b52b7d265f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6583c-9490-4085-afbe-7cff9f99c14e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace659cc-5c78-4b3c-b486-2804b1e71d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.nunique() # nunique permet de compter le nombre des valeurs uniques dans DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe89303-9c71-434c-bcf9-f4fcb19270ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['User'].unique()) # unique renvoie des valeurs uniques comme un Numpy array\n",
    "print(\"number of different users in this Twitter corpus is: \", len(df['User'].unique())) \n",
    "print(\"number of different languages used in this Twitter corpus is: \", len(df['Langue'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4402d9-4b6b-47ba-bf67-3830433d4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérifions combien sont en français ?\n",
    "df.groupby(['Langue']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737ff87-9f65-4ff7-8757-8cba189b2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Langue']=='fr'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ccb7e-cef8-40d0-8b24-39c07e1aa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une copie de l'objet en incluant les données et l'index\n",
    "# (deep copy) : utile car les changements effectués dans l'original ne sont pas reflétés dans la copie\n",
    "df_nd = df[['TweetID', 'Date', 'User', 'Tweet', 'Langue']].copy() \n",
    "df_nd.shape\n",
    "print(df_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0c10d-6178-4a24-bc9e-0df65a7e3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nd = df_nd[df_nd['Langue']=='fr']\n",
    "df_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0922cce-914f-4d87-b3b1-a511b9cf1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_duplicates : enlever les lignes dupliquées\n",
    "df_nd = df_nd.drop_duplicates().copy()\n",
    "df_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a5976-e242-4d3e-b80c-3b0a9b81812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b11a4-c0ef-471a-986e-dd9f6bb444d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nd = df_nd.drop(columns=['Langue'])\n",
    "df_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7766cfa-8b67-4af4-abac-49cedc3a7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d40587-da7e-4ae9-ba1c-527d475549fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data : lenght of the Tweets\n",
    "df_nd['char_len'] = df_nd['Tweet'].apply(lambda x:len(x))\n",
    "df_nd['word_len'] = df_nd['Tweet'].apply(lambda x:len(x.split(\" \")))\n",
    "df_nd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbd5e9-8342-412a-908c-a5a2e0f577cf",
   "metadata": {},
   "source": [
    "#### Visualison la densité du corpus !\n",
    "https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n",
    "- longueur des Tweets \n",
    "- densité : Un graphique d'estimation de la densité du noyau (KDE) est une méthode de visualisation de la distribution des observations dans un ensemble de données, analogue à un histogramme. L'estimation de la densité du noyau représente les données à l'aide d'une courbe de densité de probabilité continue dans une ou plusieurs dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48feeb3-4b4b-43ad-bd4d-8605b652eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.displot(data = df_nd, x='char_len', kde=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f14d59-749f-40fb-a1f0-8af672348079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions.\n",
    "# https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n",
    "\n",
    "sns.displot(data = df_nd, x='word_len', kde=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd27bbc-60a7-49e2-b1b9-9c5e8a969f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c095dd-f9a6-4999-bf06-e3a8e9429f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
