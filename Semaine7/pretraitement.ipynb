{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Twitter Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √©tape 0: Load necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt package\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for expanding contraction words e.g. isn't --> is not\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "stopwords.words('french')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet lemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √©tape 1 : Basic Cleaning - √† adapter √† votre corpus/imagination !\n",
    "\n",
    "- Remove Unicode Strings and Noise\n",
    "- Remove/Replace URLs, User Mentions and Hashtags\n",
    "- Non-Letter characters: numbers, emojis, or hash marks.\n",
    "- Remove/Replace Slang and Abbreviations\n",
    "- Remove/Replace Contractions\n",
    "- Remove/Replace Numbers\n",
    "- Remove/Replace Repetitions of Punctuation\n",
    "- Remove Punctuation\n",
    "- Handling Capitalized Words / Lowercase\n",
    "- Replace Elongated Words (ex: hahahaaaa, ‚ÄòDuuuuude, that's awful,‚Äô‚Äù)\n",
    "\n",
    "https://pynative.com/python-regex-replace-re-sub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following pre-tokenization receives string as input parameter\n",
    "#and returns string as output\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet) # remove Twitter links\n",
    "    return tweet\n",
    "\n",
    "def remove_tags(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    tweet = re.sub('RT @[\\w_]+:','', tweet)  # remove retweet label\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "punctuation = '!‚Äù$%&\\\"‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'\n",
    "def remove_nonText(tweet):\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub(r'\\n','', tweet)  # remove escape sequence\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet) # un exemple d'image que vous pouvez compl√©ter !\n",
    "    return tweet\n",
    "\n",
    "def remove_contraction(tweet): #enlever les contractions ou les abr√©viations\n",
    "    # tweet = ' '.join([contractions.fix(word) for word in tweet.split()])\n",
    "    contraction_dict = {\"t'es\" : \"tu es\", \"c'est\": \"ce est\", \"c\":\"ce est\"} # ajouter d'autres si n√©cessaire\n",
    "    tweet = \" \".join([contraction_dict.get(i,i) for i in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "def pretokenization_cleaning(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = remove_contraction(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Attention changer le chemin d'acc√©es √† votre fichier\n",
    "df = pd.read_csv('../Semaine6/tweets_Greve_2023.csv',encoding='utf-8')\n",
    "df_fr = df[df['Langue'] == 'fr'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = df_fr['Tweet'][4]\n",
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenization_cleaning(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling pretokenization_cleaning\n",
    "df_fr['Clean']=[pretokenization_cleaning(sentence) for sentence in df_fr['Tweet']]\n",
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √©tape 2 : Normalising data  - √† adapter √† votre corpus \n",
    "- Spelling Correction\n",
    "- Replace Negations with Antonyms\n",
    "- Handling Capitalized Words\n",
    "- Lowercase\n",
    "- Tokenization\n",
    "- Remove Stopwords (ex: the, and‚Ä¶.)\n",
    "- Stemming\n",
    "- Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer # la diff√©rence avec word_tokenize est que tweettokenizer garde les hashtags tandis que word_tokenize ne le permet pas.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer(reduce_len=True) \n",
    "    # reduce_len pour sp√©cifier s'il faut remplacer les s√©quences de caract√®res r√©p√©t√©es de longueur 3 ou plus par des s√©quences de longueur de 3\n",
    "    # par exemple pour une s√©quence 'This is waaaaayyyy too much for you!!!!!!',    waaaaayyyy -> waaayyy\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    return \" \".join([token for token in text if token.lower() not in stop_words])\n",
    "    #return [token for token in text if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean and normalizing tweets, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = remove_contraction(tweet)\n",
    "\n",
    "    ######################################\n",
    "    \n",
    "    tweet = tokenize(tweet)  # apply tokenization\n",
    "    tweet = remove_stopwords(tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling pretokenization_cleaning\n",
    "df_fr['Normalized'] = [preprocess_tweet(sentence) for sentence in df_fr['Tweet']]\n",
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a text with all words\n",
    "all_words = ' '.join([word for word in df_fr['Tweet']])\n",
    "all_Clean_words = ' '.join([word for word in df_fr['Clean']])\n",
    "all_Normalized_words = ' '.join([word for word in df_fr['Normalized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Tokenize all_words\n",
    "tokenized_words = word_tokenize(all_words)\n",
    "tokenized_Clean_words = word_tokenize(all_Clean_words)\n",
    "tokenized_Normalized_words = word_tokenize(all_Normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = tokenize(all_words)\n",
    "tokenized_Clean_words = tokenize(all_Clean_words)\n",
    "tokenized_Normalized_words = tokenize(all_Normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist_all = FreqDist(tokenized_words)\n",
    "fdist_clean = FreqDist(tokenized_Clean_words)\n",
    "fdist_normalized = FreqDist(tokenized_Normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most frequent words in the corpus:\")\n",
    "print(fdist_all.most_common(20))\n",
    "print(\"The most frequent words in the corpus---- after cleaning the data:\")\n",
    "print(fdist_clean.most_common(20))\n",
    "print(\"The most frequent words in the corpus---- after normalizing the data!\")\n",
    "print(fdist_normalized.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_normalized.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating FreqDist for whole BoW, keeping the 20 most common tokens\n",
    "all_fdist = fdist_normalized.most_common(20)\n",
    "\n",
    "## Conversion to Pandas series via Python Dictionary for easier plotting\n",
    "all_fdist = pd.Series(dict(all_fdist))\n",
    "\n",
    "## Setting figure, ax into variables\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=200,\n",
    "                      random_state=1, background_color='White',\n",
    "                      collocations=False, stopwords = stop_words).generate(all_Normalized_words)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "    # tokens = tokenize(sentence)\n",
    "    return [WordNetLemmatizer().lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "#WordNetLemmatizer().lemmatize(token, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr['Lemmatized'] = [lemmatize(sentence) for sentence in df_fr['Normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "df_fr['Lemmatized_bis'] = \"\"\n",
    "for sentence in df_fr['Lemmatized']:\n",
    "    df_fr['Lemmatized_bis'][i] = ' '.join(word for word in sentence)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Lemmatized_words = ' '.join(words for words in df_fr['Lemmatized_bis'])\n",
    "tokenized_Lemmatized_words = tokenize(all_Lemmatized_words)\n",
    "fdist_Lemmatized = FreqDist(tokenized_Lemmatized_words)\n",
    "print(\"The most frequent lemma in the corpus---- after normalizing the data!\")\n",
    "print(fdist_Lemmatized.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Obtain top 10 words\n",
    "top_20 = fdist_Lemmatized.most_common(20)\n",
    "\n",
    "# Create pandas series to make plotting easier\n",
    "fdist = pd.Series(dict(top_20))\n",
    "\n",
    "sns.barplot(y=fdist.index, x=fdist.values, color='blue');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
