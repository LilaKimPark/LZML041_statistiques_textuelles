{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Twitter Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √©tape 0: Load necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# punkt package\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# punkt package\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for expanding contraction words e.g. isn't --> is not\n",
    "# !pip install contractions\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet) # remove Twitter links\n",
    "    return tweet\n",
    "\n",
    "def remove_tags(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    tweet = re.sub('RT @[\\w_]+:','', tweet)  # remove retweet label\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "punctuation = '!‚Äù$%&\\\"‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'\n",
    "def remove_nonText(tweet):\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub(r'\\n','', tweet)  # remove escape sequence\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet) # un exemple d'image que vous pouvez compl√©ter !\n",
    "    return tweet\n",
    "\n",
    "def remove_contraction(tweet): #enlever les contractions ou les abr√©viations\n",
    "    # tweet = ' '.join([contractions.fix(word) for word in tweet.split()])\n",
    "    contraction_dict = {\"t'es\" : \"tu es\", \"c'est\": \"ce est\", \"c\":\"ce est\"} # ajouter d'autres si n√©cessaire\n",
    "    tweet = \" \".join([contraction_dict.get(i,i) for i in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer(reduce_len=True) \n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    return \" \".join([token for token in text if token.lower() not in stop_words])\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean and normalizing tweets, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = remove_contraction(tweet)\n",
    "\n",
    "    ######################################\n",
    "\n",
    "    tweet = tokenize(tweet)  # apply tokenization\n",
    "    tweet = remove_stopwords(tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Semaine6/tweets_Greve_2023.csv',encoding='utf-8')\n",
    "df_fr = df[df['Langue'] == 'fr'].copy()\n",
    "\n",
    "df_fr['Normalized'] = [preprocess_tweet(sentence) for sentence in df_fr['Tweet']]\n",
    "print(df_fr.head())\n",
    "\n",
    "all_Normalized_words = ' '.join([word for word in df_fr['Normalized']])\n",
    "tokenized_Normalized_words = word_tokenize(all_Normalized_words)\n",
    "tokenized_Normalized_words = tokenize(all_Normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "fdist_normalized = FreqDist(tokenized_Normalized_words)\n",
    "\n",
    "print(\"The most frequent words in the corpus---- after normalizing the data!\")\n",
    "print(fdist_normalized.most_common(20))\n",
    "\n",
    "fdist_normalized.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loi de Zipf\n",
    "Les distributions comme celles pr√©sent√©es ci-dessus sont typiques du langage. En r√©alit√©, ces distributions en longue tra√Æne sont si courantes dans n'importe quel corpus de langage naturel (un livre, un ensemble de textes d'un site web ou des paroles enregistr√©es) que la relation entre la fr√©quence d'utilisation d'un mot et son rang a √©t√© largement √©tudi√©e. Une version classique de cette relation est connue sous le nom de loi de Zipf, du nom de George Zipf, un linguiste am√©ricain du XX·µâ si√®cle.\n",
    "\n",
    "https://www.youtube.com/watch?v=fCn8zs912OE&ab_channel=Vsauce\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/\n",
    "\n",
    "La loi de Zipf stipule que la fr√©quence d‚Äôapparition d‚Äôun mot est inversement proportionnelle √† son rang. Ainsi, le mot le plus fr√©quent appara√Æt deux fois plus souvent que le deuxi√®me mot le plus fr√©quent, trois fois plus souvent que le troisi√®me, et ainsi de suite, jusqu‚Äôau mot le moins fr√©quent. Plus pr√©cis√©ment, un mot situ√© √† la position n appara√Æt 1/n fois aussi souvent que le mot le plus fr√©quent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating FreqDist for whole BoW, keeping the 20 most common tokens\n",
    "all_fdist = fdist_normalized.most_common(20)\n",
    "\n",
    "## Conversion to Pandas series via Python Dictionary for easier plotting\n",
    "all_fdist = pd.Series(dict(all_fdist))\n",
    "\n",
    "## Setting figure, ax into variables\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=200,\n",
    "                      random_state=1, background_color='White',\n",
    "                      collocations=False, stopwords = stop_words).generate(all_Normalized_words)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Pour r√©duire la complexit√© d‚Äôun texte, on peut \"r√©duire\" les formes d√©riv√©es √† leur forme la plus simple.\n",
    "\n",
    "Il existe deux approches dans le domaine :\n",
    "\n",
    "- la lemmatisation : un lemme correspond √† la plus petite unit√© lexicale faisant sens, qui est souvent l‚Äô√©quivalent √† une entr√©e dans le dictionnaire.\n",
    "- la racinisation (stemming) plus fruste mais plus rapide. Le terme est d√©pouill√© de ses pr√©fixes et suffixes, ainsi que de toutes ses d√©clinaisons. Souvent, cette racine ne correspond pas √† un ‚Äúvrai mot‚Äù‚Ä¶\n",
    "\n",
    "Attention il n'existe pas de fonction de lemmatisation de corpus fran√ßais dans NLTK, un bel objectif pour vos futurs projets !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "all_Normalized_stemmed = [stemmer.stem(word) for word in tokenized_Normalized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_Normalized_stemmed[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "fdist_stemm = FreqDist(all_Normalized_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most frequent words in the corpus---- after normalizing the data!\")\n",
    "print(fdist_normalized.most_common(20))\n",
    "print(\"The most frequent words in the corpus---- after stemming the data!\")\n",
    "print(fdist_stemm.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer2 = FrenchStemmer()\n",
    "all_Normalized_stemmed2 = [stemmer2.stem(word) for word in tokenized_Normalized_words]\n",
    "fdist_stemm = FreqDist(all_Normalized_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most frequent words in the corpus---- after normalizing the data!\")\n",
    "print(fdist_normalized.most_common(20))\n",
    "print(\"---------------------------------\")\n",
    "print(\"The most frequent words in the corpus---- after stemming the data!\")\n",
    "print(fdist_stemm.most_common(20))\n",
    "print(\"---------------------------------\")\n",
    "print(\"The most frequent words in the corpus---- after stemming the data with French Stemmer\")\n",
    "print(fdist_stemm.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization in Engligh in NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# create an object of class WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"takes\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"taken\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"take\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"taking\", 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librarie Spacy \n",
    "SpaCy est une biblioth√®que Python open source pour le traitement du texte et des langues naturelles. Cr√©√© par l‚Äô√©quipe de la startup Explosion AI, spaCy a √©t√© publi√©e sous la licence MIT en 2015 et ses principaux d√©veloppeurs sont Matthew Honnibal et Ines Montani, les fondateurs de la startup.\n",
    "- Contrairement √† NLTK, qui est largement utilis√© pour l‚Äôenseignement et la recherche, spaCy est con√ßu pour l‚Äôutilisation de production sur de grandes quantit√©s de textes avec une excellente performance et pr√©cision\n",
    "- Il prend en charge des mod√®les statistiques pour 21 langues dont le fran√ßais, l‚Äôanglais, l‚Äôallemand, l‚Äôespagnol, l‚Äôitalien, le portugais et le n√©erlandais. \n",
    "- Vous pouvez √©couter le tuto de Cl√©ment Plancq ! https://github.com/clement-plancq/tuto-mate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"Z58g33GglR0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installer Spacy dans terminal\n",
    "#!conda install -c conda-forge spacy\n",
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download fr_core_news_md\n",
    "#!conda install -c conda-forge spacy-model-fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_Normalized_words[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(all_Normalized_words[0:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[0:15]:\n",
    "    print((token.text, token.lemma_, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[2].text, doc[2].morph)  # 'Case=Nom|Number=Sing|Person=2|PronType=Prs'\n",
    "print(doc[2].text, doc[2].pos_) # 'PRON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Normalized_lemma = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_lemma = FreqDist(all_Normalized_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most frequent words in the corpus---- after normalizing the data!\")\n",
    "print(fdist_normalized.most_common(20))\n",
    "print(\"---------------------------------\")\n",
    "print(\"The most frequent words in the corpus---- after stemming the data!\")\n",
    "print(fdist_stemm.most_common(20))\n",
    "print(\"---------------------------------\")\n",
    "print(\"The most frequent words in the corpus---- after lemmatizing the data with spacy lemma\")\n",
    "print(fdist_lemma.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmas_words = ' '.join([word for word in all_Normalized_lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=500,\n",
    "                      random_state=1, background_color='White',\n",
    "                      collocations=False).generate(all_lemmas_words)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Obtain top 10 words\n",
    "top_20 = fdist_lemma.most_common(50)\n",
    "\n",
    "# Create pandas series to make plotting easier\n",
    "fdist = pd.Series(dict(top_20))\n",
    "\n",
    "sns.barplot(y=fdist.index, x=fdist.values, color='red');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Keywords Extraction with TF-IDF\n",
    "La recherche de \"Mots-cl√©s ‚Äì fouille de textes\" consiste √† identifier les termes ou concepts les plus r√©currents dans un ensemble de donn√©es. Ceci peut s‚Äôav√©rer tr√®s utile pour analyser les avis de clients ou les conversations sur les r√©seaux sociaux..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - TF-IDF : qu‚Äôest-ce que c‚Äôest ?\n",
    "\n",
    "TF-IDF acronyme de ¬´ Terme Frequency - Inverse Document Frequency ¬ª. Il s‚Äôagit d‚Äôun score qui permet de quantifier l‚Äôimportance d‚Äôun terme pour un document particulier compte tenu de ses occurrences dans tous les autres documents. De fait, un terme est d‚Äôautant plus pertinent s‚Äôil est surrepr√©sent√© dans un document, tout en √©tant rare dans l‚Äôensemble du corpus. Il s‚Äôagit de la m√©thode utilis√©e par les moteurs de recherches sur internet (ex : Google) pour r√©pondre √† une recherche de mots cl√©s de ses utilisateurs. TF-IDF est aujourd‚Äôhui la r√©f√©rence pour l‚Äôanalyse de textes mal structur√©s (tweets, dialogue tchat...). Ce score est calcul√© √† partir de deux composantes :\n",
    "- 1) TF : Une fonction croissante de la fr√©quence du terme dans le document √† l‚Äô√©tude.\n",
    "\n",
    "--->    Nombre d‚Äôoccurrence du terme analys√© / Nombre de termes total.\n",
    "- 2) IDF : Une fonction inversement proportionnelle √† la fr√©quence du terme dans le corpus.\n",
    "\n",
    "--->    Log(Nombre total de documents / Nombre de documents contenant le terme analys√©)\n",
    "\n",
    "- Le score TF-IDF est le r√©sultat de la multiplication des deux termes TF et IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions used for TF IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "Collection = [ ]\n",
    "df_tfidf = pd.DataFrame()\n",
    "Collection = df_fr['Normalized'].to_list()\n",
    "\n",
    "vectorizer = TfidfVectorizer() #stop_words=stop_words_french)\n",
    "vectors = vectorizer.fit_transform(Collection)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "tf_idf_vector = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Now you can create an inverse mapping:\n",
    "m = {v: k for (k, v) in vectorizer.vocabulary_.items()}\n",
    "\n",
    "#and this gives the influential word per doc:\n",
    "keyword=[m[t] for t in np.array(np.argmax(vectors, axis=1)).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Most Significant Term for each Document¬∂\n",
    "keyword[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tfidf=[]\n",
    "Top_n=100\n",
    "for i in range(len(Collection)):\n",
    "    doc=tf_idf_vector.iloc[i].to_numpy()\n",
    "    list_tfidf.append([m[t] for t in np.array(np.argsort(-doc)).flatten()[0:Top_n]])\n",
    "df_tfidf=pd.DataFrame(list_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 3 top words in each document\n",
    "top_dict = {}\n",
    "data = tf_idf_vector.transpose()\n",
    "\n",
    "for i, c in enumerate(data.columns):\n",
    "    top = data.loc[:,c].sort_values(ascending=False).head(50)\n",
    "    top_dict[data.columns[i]]= list(zip(top.index, top.values))\n",
    "\n",
    "# Print the top 3 words said by tweet\n",
    "for tweet, top_words in top_dict.items():\n",
    "    print(tweet)\n",
    "    print(', '.join([word for word, count in top_words[0:3]]))\n",
    "    print(', '.join([str(count) for word, count in top_words[0:3]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking documents by TF-IDF Score for a Search Word¬∂\n",
    "query = \"gr√®ve\"\n",
    "tf_idf_vector.sort_values(by=[query], ascending=True)[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of DataFrame\n",
    "tf_idf_vector.sort_values(by=[query], ascending=False).index[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le mot recherch√©\n",
    "query = \"gr√®ve\"\n",
    "# tri√© la colonne correspondant √† ce terme dans la matrice des scores TF-IDF\n",
    "index = tf_idf_vector.sort_values(by=[query], ascending=False).index.to_list()\n",
    "#Utiliser les index pour trouver les Tweets les plus \n",
    "for i in range(15):\n",
    "    print(\"Tweet #\", index[i], \":\", df_fr['Tweet'][index[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
